{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Function softmax\n",
    "\n",
    "## Definicion\n",
    "La funcion softmax tambien se conoce con el nombre de funcion exponencial normalizada.  Tambien se suele llamar a la funcion softmax como funcion de generalizacion de la funcion logistica.  De forma general, la funcion logistica es una funcion matematica que se ha utilizado como refinamiento del modelo exponencial.     Un modelo exponencial es aplicado para resolver problemas sobre el crecimiento de poblaciones, propagacion de enfermedades y difusion de redes sociales.  El comportamiento de una funcion logistica tiene tres etapas, inicial, crecimiento y madurez.   En la fase inicial el crecimiento se aproxima a ser exponencial, en la fase de crecimiento los miembros compiten por los recursos criticos y en la fase de madurez, los miembros provocan un cuello de botella que disminuye el crecimiento hasta detenerse.     En la Figura 1, puede apreciarse el comportamiento de la funcion softmax.\n",
    "\n",
    "\n",
    "Figura 1. Grafica de la funcion softmax\n",
    "\n",
    "![](https://qph.ec.quoracdn.net/main-qimg-fda2f008df90ed5d7b6aff89b881e1ac)\n",
    "\n",
    "\n",
    "# Sintaxis\n",
    "\n",
    "En tensorFlow 2,017 su definicion es calcula las activaciones softmax, una defincion muy basica para comprender su proposito.  La funcion realiza el equivalente de:\n",
    "```\n",
    "softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), dim)\n",
    "```\n",
    "\n",
    "El codigo de la funcion softmax en tensorflow es el siguiente:\n",
    "```\n",
    "softmax(logits,dim=-1,name=None)\n",
    "```\n",
    "\n",
    "Luego de conocer que la funcion de activacion softmax proviene de una funcion logistica y esta a su vez es el refinamiento del modelo exponencial, se puede enteder con claridad cual es su comportamiento.\n",
    "\n",
    "En la Figura 2, se puede observar que la funcion de activacion softmax es utilizada para normalizar los valores de la ultima capa, que frecuentemente es una capa totalmente conectada (full connected layer)\n",
    "\n",
    "\n",
    "Figura 2. Uso de funcion softmax en una red nuronal convolucional\n",
    "\n",
    "![](https://sebastianraschka.com/images/faq/softmax_regression/logistic_regression_schematic.png)\n",
    "\n",
    "## Objetivo\n",
    "El objetivo de la funcion softmax es comprimir un vector de k-dimensiones de valores reales y convertir en un vector de k-dimensiones (las mismas dimensiones) de valores reales contenidos en el rango [0-1].\n",
    "\n",
    "## Proposito\n",
    "Su proposito es normalizar los valores de un tensor, es decir, proporcionar el resultado en un rango de [0-1], incluyendo valores negativos y positivos.\n",
    "Segun cv-tricks en 2,017, la funcion softmax convierte un vector de k-dimensiones de valores reales a un vector en la misma forma de valores reales en un rango de [0-1] que suman 1.  Deberiamos aplicar la funcion softmax al resultado de nuestra red neuronal convolucional a fin de convertir el resultado a una probabilidad para cada clase.   Es decir que si tenemos tres clases o tres tipos de objectos que la red neuronal reconoce, entonces la sumatoria de las probabilidades de cada clase debe sumar 1.\n",
    "\n",
    "sebastianraschka.com en 2,017, hace tambien su aporte describiendo a la funcion softmax como una funcion logistica de regresion multi-clase, utilizada para clasificacion mult-clases.\n",
    "\n",
    "Los resultados de la funcion softmax serviran para clasificar determinar la probabilidad que una nueva imagen pertenezca a una de las clases de nuestro resultados de la funcion softmax, en la Figura 3 se puede observar que una imagen de un gato es analizada y el modelo retorna la probabilidad de que sea un gato o un perro.\n",
    "\n",
    "Figura 3. Uso de la funcion softmax.\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*eEKb2RxREV6-MtLz2DNWFQ.gif)\n",
    "\n",
    "## Argumentos de la funcion en tensorFlow\n",
    "Para ejecutar la funcion en tensorflow necesitamos conocer cuales son los parametros de entrada y como utilizarlos.\n",
    "1. logits: es un tensor no vacio.  El tipo de datos puede ser half, float32 o float64. logits significa que los valores de entrada pueden ser valores no normalizados, es decir que puede ser cualquier numero, no solo entre el rango [0-1]\n",
    "2. dim: la dimension softmax podria ser realizada en, el valor default es -1 el cual indica la ultima dimension.\n",
    "3. name: un nombre para la operacion, este argumento es opcional.\n",
    "\n",
    "\n",
    "# Referencias\n",
    "* [wikipedia.org (2,017). Funcion softmax](https://es.wikipedia.org/wiki/Funci%C3%B3n_SoftMax)\n",
    "* [wikipedia.org (2,017). Funcion logistica](https://es.wikipedia.org/wiki/Funci%C3%B3n_log%C3%ADstica)\n",
    "* [wikipedia.org (2,017). Modelo exponencial](https://es.wikipedia.org/wiki/Modelo_exponencial)\n",
    "* [cv-tricks.com (2,017). Tensorflow Tutorial 2: image classifier using convolutional neural network](http://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/)\n",
    "* [tensorflow.org (2,017). tf.nn.softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)\n",
    "* [Stackoverflow.com (2,017). What's the difference between softmax and softmax_cross entropy with logits?](https://stackoverflow.com/questions/34240703/whats-the-difference-between-softmax-and-softmax-cross-entropy-with-logits)\n",
    "* [What is Softmax regression and how is it related to Logistic regression?](https://sebastianraschka.com/faq/docs/softmax_regression.html)\n",
    "* [What the Hell is Perceptron?](https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original values of of one dimension \n",
      "[-0.2  0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1. ]\n",
      "\n",
      "Result one dimension: \n",
      "[[ 0.04119399  0.05031445  0.05560607  0.06145421  0.0679174   0.07506034\n",
      "   0.0829545   0.0916789   0.10132086  0.11197686  0.12375357  0.13676885]]\n",
      "\n",
      "Result two dimension: \n",
      "[[ 0.11717936  0.14312319  0.15817559  0.17481106  0.21351471  0.1931961 ]\n",
      " [ 0.07450818  0.15004106  0.16582101  0.18326056  0.20253424  0.22383495]]\n",
      "\n",
      "Flatten result of one dimension \n",
      "[11]\n",
      "\n",
      "Flatten result of two dimension \n",
      "[4 5]\n",
      "\n",
      "2. CHECK RESULT OF SOFTMAX FUNCTION STEP BY STEP \n",
      "\n",
      "Return e elevated to the power of each number \n",
      " [0.8187307530779818, 1.0, 1.1051709180756477, 1.2214027581601699, 1.3498588075760032, 1.4918246976412703, 1.6487212707001282, 1.8221188003905089, 2.0137527074704766, 2.225540928492468, 2.45960311115695, 2.718281828459045]\n",
      "\n",
      "sum the result of elevate e to the power of each number \n",
      "19.875006581200648\n",
      "\n",
      "softmax values \n",
      "[0.041, 0.05, 0.056, 0.061, 0.068, 0.075, 0.083, 0.092, 0.101, 0.112, 0.124, 0.137]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "# CREATE SESSION\n",
    "session = tf.Session()\n",
    "#CREATE VARIABLES AND PLACEHOLDERS\n",
    "valuesOneDimension = tf.constant(np.array([[-.2, .0, .1, .2, .3, 0.4, .5, .6, .7, .8, .9 , 1]]))\n",
    "valuesTwoDimension = tf.constant(np.array([[-.2, .0, .1, .2, .4, 0.3],[-.2,.5, .6, .7, .8, .9]]))\n",
    "#INIT VARIABLES\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "## GET VALUES OF valuesOneDimension AND TO SAVE IN valuesOneDimensionArray\n",
    "valuesOneDimensionArray = session.run(valuesOneDimension[0])\n",
    "print(\"original values of of one dimension \\n\"+str(valuesOneDimensionArray)+\"\\n\")\n",
    "\n",
    "#RUN SESSION\n",
    "resultOneDimension = session.run(tf.nn.softmax(valuesOneDimension))\n",
    "resultTwoDimension = session.run(tf.nn.softmax(valuesTwoDimension))\n",
    "flattenedResultOfOneDimension =session.run(tf.argmax(resultOneDimension, axis=1))\n",
    "flattenedResultOfTwoDimension =session.run(tf.argmax(resultTwoDimension, axis=1))\n",
    "\n",
    "#PRINT RESULTS.\n",
    "print(\"Result one dimension: \\n\"+str(resultOneDimension)+\"\\n\")\n",
    "print(\"Result two dimension: \\n\"+str(resultTwoDimension)+\"\\n\")\n",
    "print(\"Flatten result of one dimension \\n\"+str(flattenedResultOfOneDimension)+\"\\n\")\n",
    "print(\"Flatten result of two dimension \\n\"+str(flattenedResultOfTwoDimension)+\"\\n\")\n",
    "\n",
    "#CHECK RESULT STEP BY STEP\n",
    "print(\"2. CHECK RESULT OF SOFTMAX FUNCTION STEP BY STEP \\n\")\n",
    "\n",
    "# GET VALUES WITH ONE DIMENSION IN AN ARRAY\n",
    "valuesOneDimensionExponential = [math.exp(i) for i in valuesOneDimensionArray]\n",
    "print(\"Return e elevated to the power of each number \\n \"+str(valuesOneDimensionExponential)+\"\\n\")\n",
    "\n",
    "SumValuesOneDimensionExponential = sum(valuesOneDimensionExponential)\n",
    "print(\"sum the result of elevate e to the power of each number \\n\"+str(SumValuesOneDimensionExponential)+\"\\n\")\n",
    "\n",
    "softmax = [round(number / SumValuesOneDimensionExponential, 3) for number in valuesOneDimensionExponential]\n",
    "print(\"softmax values \\n\"+str(softmax)+\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}